{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - 9th March"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Mass Function (PMF) and Probability Density Function (PDF)**\n",
    "\n",
    "The Probability Mass Function (PMF) and Probability Density Function (PDF) are both ways of describing the distribution of a random variable. However, they are used for different types of random variables.\n",
    "\n",
    "**PMF**\n",
    "\n",
    "The PMF is used for discrete random variables, which can take on a finite or countably infinite set of values. The PMF describes the probability of each possible value that the random variable can take. Mathematically, the PMF is defined as follows:\n",
    "\n",
    "PMF(x) = P(X = x)\n",
    "\n",
    "where X is the random variable and x is a particular value that X can take. The PMF gives the probability that X takes the value x.\n",
    "\n",
    "For example, consider a fair six-sided die. The random variable X represents the outcome of rolling the die. The PMF of X is given by:\n",
    "\n",
    "|x|PMF(x)|\n",
    "|---|---|\n",
    "|1|1/6 |\n",
    "|2|1/6 |\n",
    "|3|1/6 |\n",
    "|4|1/6 |\n",
    "|5|1/6 |\n",
    "|6|1/6 |\n",
    "\n",
    "This PMF tells us that the probability of rolling a 1, 2, 3, 4, 5, or 6 is each 1/6.\n",
    "\n",
    "**PDF**\n",
    "\n",
    "The PDF, on the other hand, is used for continuous random variables, which can take on any value in a continuous range. The PDF describes the relative likelihood of the random variable taking on a particular value or range of values. Mathematically, the PDF is defined as follows:\n",
    "\n",
    "$\\int_{a}^{b} f(x) dx = P(a \\leq X \\leq b)$\n",
    "\n",
    "where X is the random variable, f(x) is the PDF, and a and b are the lower and upper bounds of the range of interest. The PDF gives the probability density, which is the likelihood of the random variable taking on values in the range (a, b).\n",
    "\n",
    "For example, consider the normal distribution with mean $\\mu = 0$ and variance $\\sigma^2 = 1$. The PDF of this distribution is given by:\n",
    "\n",
    "$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "This PDF describes the relative likelihood of the random variable taking on different values. The highest likelihood is around x=0, which corresponds to the mean of the distribution. As we move away from the mean, the likelihood decreases, but never reaches zero. The area under the PDF curve between any two values of x gives the probability that the random variable takes on a value in that range.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cumulative Density Function (CDF)**\n",
    "\n",
    "The Cumulative Density Function (CDF) is a function used to describe the probability of a random variable taking on a value less than or equal to a certain value. In other words, it gives the cumulative probability up to a certain point. \n",
    "\n",
    "The CDF is defined for both continuous and discrete random variables. For a continuous random variable, the CDF is the integral of the probability density function (PDF) from negative infinity to the point of interest. For a discrete random variable, the CDF is the sum of the probability mass function (PMF) up to the point of interest.\n",
    "\n",
    "The CDF is used to calculate a variety of important statistics and properties of a probability distribution, such as the expected value, variance, and percentiles. It is also useful for generating random numbers from a given probability distribution using techniques such as inverse transform sampling or the acceptance-rejection method.\n",
    "\n",
    "**For Example :**\n",
    "Suppose we have a continuous random variable X that follows a normal distribution with a mean of 10 and a standard deviation of 2. We can use the CDF to find the probability that X is less than or equal to 12.\n",
    "\n",
    "The PDF for a normal distribution is given by:\n",
    "\n",
    "$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "\n",
    "Using this formula, we can calculate the CDF as follows:\n",
    "\n",
    "$F(x) = P(X \\leq x) = \\int_{-\\infty}^x f(x) dx$\n",
    "\n",
    "For our example, we have:\n",
    "\n",
    "$F(12) = P(X \\leq 12) = \\int_{-\\infty}^{12} \\frac{1}{\\sqrt{2\\pi(2)^2}} e^{-\\frac{(x-10)^2}{2(2)^2}} dx \\approx 0.9772$\n",
    "\n",
    "Therefore, the probability that X is less than or equal to 12 is approximately 0.9772."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of Using the Normal Distribution**\n",
    "\n",
    "The normal distribution, also known as the Gaussian distribution, is a very common probability distribution that is used to model a wide range of natural and social phenomena. Some examples of situations where the normal distribution might be used as a model include:\n",
    "\n",
    "- Heights or weights of people in a population\n",
    "- IQ scores or other measures of cognitive ability\n",
    "- Measurement errors in scientific experiments\n",
    "- Stock prices or other financial data\n",
    "- Test scores or grades in education\n",
    "- Blood pressure or other physiological measures\n",
    "\n",
    "The normal distribution is characterized by two parameters: the mean ($\\mu$) and the standard deviation ($\\sigma$). The mean determines the center of the distribution, while the standard deviation determines the spread or variability of the distribution. \n",
    "\n",
    "The normal distribution is symmetric around the mean, with most of the data falling within 1, 2, or 3 standard deviations from the mean. The area under the curve of the normal distribution is equal to 1, which means that the total probability of all possible events in the distribution is 1.\n",
    "\n",
    "The normal distribution is a continuous probability distribution, which means that it can take on any value within a certain range. The shape of the distribution is bell-shaped, with the tails of the distribution extending infinitely in either direction.\n",
    "\n",
    "**Note :** The normal distribution is a versatile and widely used tool in statistics and data analysis. It is characterized by its mean and standard deviation, which determine the center and variability of the distribution, respectively.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution.** \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance of Normal Distribution**\n",
    "\n",
    "The normal distribution, also known as the Gaussian distribution, is one of the most important probability distributions in statistics. It is important because it arises naturally in many real-world applications, and it is also used as a theoretical model in statistical analysis.\n",
    "\n",
    "One of the key reasons for the importance of the normal distribution is the Central Limit Theorem (CLT), which states that the sum of a large number of independent and identically distributed random variables tends to be normally distributed, regardless of the distribution of the individual variables themselves. This makes the normal distribution a useful tool for modeling many real-world phenomena.\n",
    "\n",
    "**Real-life Examples of Normal Distribution**\n",
    "\n",
    "Here are a few examples of real-world phenomena that can be modeled using the normal distribution:\n",
    "\n",
    "- Heights or weights of individuals in a population: These tend to follow a normal distribution, with most people clustered around the mean and fewer people at the extremes.\n",
    "- IQ scores or test scores: These also tend to follow a normal distribution, with most people scoring around the mean and fewer people at the extremes.\n",
    "- Measurement errors in scientific experiments: These are often normally distributed around the true value, with more errors close to the true value and fewer errors farther away.\n",
    "- Stock prices: These can be modeled using a log-normal distribution, which is a variant of the normal distribution that is used to model positive quantities that can vary over several orders of magnitude.\n",
    "- Waiting times or processing times: These can often be modeled using the normal distribution, with most times clustered around the mean and fewer times at the extremes.\n",
    "\n",
    "These are just a few examples of the many real-world phenomena that can be modeled using the normal distribution. By understanding the characteristics of the normal distribution and its parameters, statisticians and data scientists can gain important insights into the data they are working with and make informed decisions based on that data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bernoulli Distribution**\n",
    "\n",
    "The Bernoulli distribution is a discrete probability distribution that describes the outcomes of a single binary event (i.e., an event that has only two possible outcomes, such as success or failure). It is named after the Swiss mathematician Jakob Bernoulli, who first used it in the 18th century to study the probability of getting a certain number of heads or tails in a coin toss.\n",
    "\n",
    "The Bernoulli distribution is characterized by a single parameter p, which represents the probability of success (i.e., the probability of getting the outcome of interest). The probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\n",
    "$P(X=x) = p^x(1-p)^{1-x}$, for $x = 0$ or $1$\n",
    "\n",
    "where X is the random variable that represents the outcome of the event, and x can take on the values of 0 or 1 (i.e., the two possible outcomes).\n",
    "\n",
    "**Example**\n",
    "\n",
    "An example of a situation where the Bernoulli distribution might be used is in modeling the outcome of a single coin toss. Let's say we have a fair coin (i.e., one that has an equal probability of coming up heads or tails), and we want to know the probability of getting heads on a single toss. In this case, the parameter p would be 0.5 (since there is a 50% chance of getting heads), and the PMF would be:\n",
    "\n",
    "$P(X=0) = 0.5^0(1-0.5)^{1-0} = 0.5^1 = 0.5$\n",
    "\n",
    "$P(X=1) = 0.5^1(1-0.5)^{1-1} = 0.5^1 = 0.5$\n",
    "\n",
    "**Difference between Bernoulli and Binomial Distribution**\n",
    "\n",
    "The Bernoulli distribution is a special case of the binomial distribution, which describes the outcomes of a fixed number of independent and identically distributed Bernoulli trials (i.e., a sequence of binary events). In other words, the binomial distribution is the sum of multiple Bernoulli trials.\n",
    "\n",
    "The main difference between the Bernoulli distribution and the binomial distribution is that the Bernoulli distribution describes the outcome of a single trial, while the binomial distribution describes the outcomes of multiple trials. The binomial distribution is characterized by two parameters: n, which represents the number of trials, and p, which represents the probability of success in each trial.\n",
    "\n",
    "The probability mass function (PMF) of the binomial distribution is given by:\n",
    "\n",
    "$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$, for $k = 0, 1, 2, ..., n$\n",
    "\n",
    "where X is the random variable that represents the number of successes in n trials, k is the number of successes, and (n choose k) is the binomial coefficient that represents the number of ways to choose k successes from n trials.\n",
    "\n",
    "In summary, the Bernoulli distribution is used to model the outcome of a single binary event, while the binomial distribution is used to model the outcomes of multiple independent and identically distributed binary events.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate this probability, we will use the Z-score formula:\n",
    "\n",
    "$Z = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "where:\n",
    "- X is the value we are interested in (in this case, $X$ = 60)\n",
    "- μ is the mean of the dataset ($\\mu$ = 50)\n",
    "- σ is the standard deviation of the dataset ($\\sigma$ = 10)\n",
    "\n",
    "Substituting the values, we get:\n",
    "\n",
    "$Z = \\frac{(60 - 50)}{10}$\n",
    "\n",
    "$Z = 1$\n",
    "\n",
    "Using a Z-score table or calculator, we can find that the probability of a Z-score being less than 1 is approximately 0.8413. Since we are interested in the probability of a value being greater than 60, we need to subtract this value from 1 to get:\n",
    "\n",
    "$P(X > 60) = 1 - 0.8413$\n",
    "\n",
    "$P(X > 60) = 0.1587$\n",
    "\n",
    "Therefore, the probability of a randomly selected observation being greater than 60 is approximately 0.1587.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Explain uniform Distribution with an example.** \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniform Distribution**\n",
    "\n",
    "Uniform distribution is a type of probability distribution where all outcomes or events are equally likely to occur. In other words, the probability of any given outcome is constant and is spread evenly across the entire range of possible values.\n",
    "\n",
    "An example of uniform distribution is rolling a fair six-sided die. Since each face of the die has an equal chance of showing up, the probability of rolling any particular number from 1 to 6 is 1/6 or approximately 0.1667.\n",
    "\n",
    "Uniform distribution is often used in simulations, random sampling, and other areas of probability and statistics where all outcomes are assumed to be equally likely.\n",
    "\n",
    "In mathematical terms, the probability density function (PDF) of a uniform distribution can be expressed as:\n",
    "\n",
    "\n",
    "$f(x) = \\frac{1}{b - a}$\n",
    "\n",
    "where:\n",
    "- x is a value within the range [a, b]\n",
    "- a is the minimum value in the range\n",
    "- b is the maximum value in the range\n",
    "\n",
    "For example, if we want to model the probability of rolling a number between 1 and 6 on a fair six-sided die, we can use a uniform distribution with $a = 1$ and $b = 6$. The PDF of this distribution would be:\n",
    "\n",
    "$f(x) = \\frac{1}{b-a} = \\frac{1}{6-1} = \\frac{1}{5}$\n",
    "\n",
    "This means that the probability of rolling any particular number between 1 and 6 is 1/5 or approximately 0.2.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **What is the z score? State the importance of the z score.** \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z-score**\n",
    "\n",
    "A z-score is a measure of how many standard deviations a given data point is from the mean of a dataset. It is also known as a standard score, because it is based on the standard deviation of the dataset.\n",
    "\n",
    "The formula for calculating the z-score of a data point is:\n",
    "\n",
    "$ z = \\frac{(x - μ)}{σ} $\n",
    "\n",
    "where:\n",
    "- x is the data point\n",
    "- μ is the mean of the dataset\n",
    "- σ is the standard deviation of the dataset\n",
    "\n",
    "The z-score tells us how many standard deviations a given data point is from the mean. If the z-score is positive, the data point is above the mean, and if it is negative, the data point is below the mean. A z-score of 0 indicates that the data point is exactly at the mean.\n",
    "\n",
    "The importance of the z-score lies in its ability to standardize data and make it easier to compare different datasets. By converting data to z-scores, we can compare data from different datasets that may have different scales or units of measurement. It also helps us identify outliers or extreme values that are far from the mean.\n",
    "\n",
    "In addition, the z-score is used in hypothesis testing and statistical inference to calculate the probability of observing a particular data point or sample mean, assuming a normal distribution of the data. The z-score can be used to calculate p-values, which tell us the likelihood of observing a particular result by chance.\n",
    "\n",
    "Overall, the z-score is an important statistical tool that helps us understand the relationship between individual data points and the distribution of the dataset as a whole. It allows us to standardize data, compare different datasets, identify outliers, and perform hypothesis testing and statistical inference.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **What is Central Limit Theorem? State the significance of the Central Limit Theorem.** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Central Limit Theorem**\n",
    "\n",
    "Central Limit Theorem (CLT) is a fundamental concept in statistics that states that the sampling distribution of the mean of any independent, random variable will be approximately normal, regardless of the distribution of the original variable, provided the sample size is large enough. \n",
    "\n",
    "The Central Limit Theorem can be stated mathematically as follows:\n",
    "\n",
    "If we have a sample of size n from any population with a mean μ and a finite variance σ^2, then the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, with a mean equal to the population mean (μ) and a standard deviation equal to the population standard deviation divided by the square root of the sample size $\\frac{\\sigma}{\\sqrt(n)}$.\n",
    "\n",
    "In other words, the Central Limit Theorem tells us that the distribution of sample means will be approximately normal, regardless of the distribution of the population from which the samples were drawn, as long as the sample size is large enough.\n",
    "\n",
    "**Significance of the Central Limit Theorem**\n",
    "\n",
    "The Central Limit Theorem is significant because it enables us to make inferences about the population based on sample data. It allows us to estimate the population parameters, such as the mean and the standard deviation, with a high degree of accuracy. \n",
    "\n",
    "Moreover, the Central Limit Theorem is the basis for many statistical tests, such as t-tests and ANOVA, which are used to compare means and variances of different populations. These tests rely on the assumption that the distribution of the population means is approximately normal, which is ensured by the Central Limit Theorem.\n",
    "\n",
    "The Central Limit Theorem is also important in quality control, finance, and many other fields where statistical analysis is necessary. It provides a solid foundation for statistical analysis and helps us draw meaningful conclusions from data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **State the assumptions of the Central Limit Theorem.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumptions of the Central Limit Theorem**\n",
    "\n",
    "The Central Limit Theorem (CLT) is based on some assumptions, which are as follows:\n",
    "\n",
    "1. **Random Sampling**: The sample should be selected randomly from the population, and each member of the population should have an equal chance of being selected.\n",
    "\n",
    "2. **Sample Size**: The sample size should be sufficiently large. The general rule of thumb is that the sample size should be greater than 30, but this may vary depending on the population distribution and the desired level of accuracy.\n",
    "\n",
    "3. **Independence**: The observations in the sample should be independent of each other. This means that the value of one observation should not be influenced by the value of another observation.\n",
    "\n",
    "4. **Finite Variance**: The population should have a finite variance. This assumption is necessary to ensure that the sample mean has a well-defined variance.\n",
    "\n",
    "If these assumptions are met, the Central Limit Theorem guarantees that the sampling distribution of the sample mean will be approximately normal, regardless of the population distribution. However, if any of these assumptions are violated, the Central Limit Theorem may not apply, and other statistical methods may be needed to analyze the data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
